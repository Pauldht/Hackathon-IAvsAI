{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Main First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run Main.ipynb\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from glob import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_wv = gensim.downloader.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 3884/227717 [00:04<04:37, 807.76it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m       \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mzeros(wv\u001b[39m.\u001b[39mvector_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(word_vectors, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([document_vector(text, pretrained_wv) \u001b[39mfor\u001b[39;49;00m text \u001b[39min\u001b[39;49;00m tqdm(X)])\n",
      "\u001b[1;32m/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m       \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mzeros(wv\u001b[39m.\u001b[39mvector_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(word_vectors, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([document_vector(text, pretrained_wv) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m tqdm(X)])\n",
      "\u001b[1;32m/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create document vectors by averaging word vectors.\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m words \u001b[39m=\u001b[39m word_tokenize(doc)\n\u001b[0;32m----> <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m word_vectors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([wv[word] \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m words \u001b[39mif\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m wv])\n\u001b[1;32m      <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(word_vectors) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mzeros(wv\u001b[39m.\u001b[39mvector_size)\n",
      "\u001b[1;32m/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create document vectors by averaging word vectors.\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m words \u001b[39m=\u001b[39m word_tokenize(doc)\n\u001b[0;32m----> <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m word_vectors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([wv[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;49;00m wv])\n\u001b[1;32m      <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(word_vectors) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mzeros(wv\u001b[39m.\u001b[39mvector_size)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/gensim/models/keyedvectors.py:649\u001b[0m, in \u001b[0;36mKeyedVectors.__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__contains__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m--> 649\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhas_index_for(key)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/gensim/models/keyedvectors.py:646\u001b[0m, in \u001b[0;36mKeyedVectors.has_index_for\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhas_index_for\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m    638\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can this model return a single index for this key?\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \n\u001b[1;32m    640\u001b[0m \u001b[39m    Subclasses that synthesize vectors for out-of-vocabulary words (like\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    644\u001b[0m \n\u001b[1;32m    645\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_index(key, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/gensim/models/keyedvectors.py:407\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_vector(key_or_keys)\n\u001b[1;32m    405\u001b[0m     \u001b[39mreturn\u001b[39;00m vstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_vector(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m key_or_keys])\n\u001b[0;32m--> 407\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_index\u001b[39m(\u001b[39mself\u001b[39m, key, default\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    408\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the integer index (slot/position) where the given key's vector is stored in the\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[39m    backing vectors array.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \n\u001b[1;32m    411\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_to_index\u001b[39m.\u001b[39mget(key, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 3884/227717 [00:16<04:37, 807.76it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def document_vector(doc, wv):\n",
    "  \"\"\"Create document vectors by averaging word vectors.\"\"\"\n",
    "  words = word_tokenize(doc)\n",
    "  word_vectors = np.array([wv[word] for word in words if word in wv])\n",
    "  \n",
    "  if len(word_vectors) == 0:\n",
    "      return np.zeros(wv.vector_size)\n",
    "  return np.mean(word_vectors, axis=0)\n",
    "  \n",
    "\n",
    "X = np.array([document_vector(text, pretrained_wv) for text in tqdm(X)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Convert to PyTorch tensors\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m X_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(X, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m      <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m y_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(y, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m      <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Split the data\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDataset objects\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron2(\n",
      "  (fc1): Linear(in_features=100, out_features=20, bias=True)\n",
      "  (norm1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (norm2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (norm3): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.2, inplace=False)\n",
      "  (fc4): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 1 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m---> <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://user-05032002-517967-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/FNN1.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 1 is out of bounds."
     ]
    }
   ],
   "source": [
    "import torch.nn.init as init\n",
    "class MultiLayerPerceptron2(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_classes):\n",
    "        super(MultiLayerPerceptron2, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_dim*2)\n",
    "        self.norm1 = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim*2, hidden_dim*2)\n",
    "        self.norm2 = nn.BatchNorm1d(hidden_dim*2)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.norm3 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout3 = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        # Initialisation des poids\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.kaiming_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x)\n",
    "        return self.fc4(x)\n",
    "\"\"\"\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x.long())\n",
    "        embedded = self.dropout(embedded)\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        return torch.sigmoid(self.fc(hidden.squeeze(0)))\n",
    "\"\"\"\n",
    "\n",
    "Train = True\n",
    "\n",
    "if (Train)\n",
    "{\n",
    "    num_epochs = 20\n",
    "    vocab_size = X_tensor.shape[1]\n",
    "    num_classes = len(set(y.values))\n",
    "\n",
    "    model = MultiLayerPerceptron2(vocab_size, 10, 2) #0.87 0.87 0.87\n",
    "    #model = Classifier(vocab_size, 2, vocab_size, num_classes) #\n",
    "    print(model)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.long()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, Value in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_labels.extend(Value)\n",
    "                all_predictions.extend(predicted)\n",
    "\n",
    "        precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "        \n",
    "        print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "        SavePath = 'model/fnn1.pth'\n",
    "\n",
    "        # Sauvegarder le mod√®le\n",
    "        torch.save(model.state_dict(), SavePath)\n",
    "}\n",
    "else\n",
    "{\n",
    "    model = MultiLayerPerceptron2(vocab_size, 20, num_classes) # Assurez-vous de cr√©er une instance du mod√®le avec les m√™mes param√®tres qu'avant la sauvegarde\n",
    "    model.load_state_dict(torch.load(\"modele/fnn1.pth\"))\n",
    "    model.eval()  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilit√©s de chaque classe : Humain : {}, GPT {} tensor([[6.0200e-42, 1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Calcul du vecteur de document et ajout d'une dimension\n",
    "doc_vector = document_vector(\"Je suis\", pretrained_wv)\n",
    "doc_vector = torch.tensor(doc_vector, dtype=torch.float32).unsqueeze(0)  # Ajout d'une dimension avec unsqueeze(0)\n",
    "\n",
    "# Passage au mod√®le\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(doc_vector)\n",
    "\n",
    "# Appliquer softmax pour obtenir des probabilit√©s\n",
    "probabilities = F.softmax(output, dim=1)  # Utilisation de dim=1 car nous voulons appliquer softmax sur les classes\n",
    "\n",
    "# Afficher les probabilit√©s\n",
    "print(\"Probabilit√©s de chaque classe : Humain : {}, GPT {}\", probabilities.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
