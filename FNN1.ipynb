{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Main First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run Main.ipynb\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from glob import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_wv = gensim.downloader.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170898/170898 [03:25<00:00, 832.25it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def document_vector(doc, wv):\n",
    "  \"\"\"Create document vectors by averaging word vectors.\"\"\"\n",
    "  words = word_tokenize(doc)\n",
    "  word_vectors = np.array([wv[word] for word in words if word in wv])\n",
    "  \n",
    "  if len(word_vectors) == 0:\n",
    "      return np.zeros(wv.vector_size)\n",
    "  return np.mean(word_vectors, axis=0)\n",
    "  \n",
    "\n",
    "X = np.array([document_vector(text, pretrained_wv) for text in tqdm(X)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDataset objects\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron2(\n",
      "  (fc1): Linear(in_features=100, out_features=4, bias=True)\n",
      "  (norm1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (fc2): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (norm2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  (fc3): Linear(in_features=4, out_features=2, bias=True)\n",
      "  (dropout3): Dropout(p=0.1, inplace=False)\n",
      "  (fc4): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n",
      "Epoch [1/20], Loss: 0.3013\n",
      "Precision: 0.8728, Recall: 0.8430, F1 Score: 0.8479\n",
      "Epoch [2/20], Loss: 0.1620\n",
      "Precision: 0.8787, Recall: 0.8797, F1 Score: 0.8790\n",
      "Epoch [3/20], Loss: 0.1292\n",
      "Precision: 0.8760, Recall: 0.8772, F1 Score: 0.8741\n",
      "Epoch [4/20], Loss: 0.2647\n",
      "Precision: 0.8876, Recall: 0.8883, F1 Score: 0.8879\n",
      "Epoch [5/20], Loss: 0.4104\n",
      "Precision: 0.8924, Recall: 0.8920, F1 Score: 0.8922\n",
      "Epoch [6/20], Loss: 0.4332\n",
      "Precision: 0.8964, Recall: 0.8942, F1 Score: 0.8950\n",
      "Epoch [7/20], Loss: 0.0838\n",
      "Precision: 0.8967, Recall: 0.8938, F1 Score: 0.8947\n",
      "Epoch [8/20], Loss: 0.6664\n",
      "Precision: 0.8983, Recall: 0.8946, F1 Score: 0.8957\n",
      "Epoch [9/20], Loss: 0.1889\n",
      "Precision: 0.8899, Recall: 0.8910, F1 Score: 0.8889\n",
      "Epoch [10/20], Loss: 0.4189\n",
      "Precision: 0.8981, Recall: 0.8949, F1 Score: 0.8959\n",
      "Epoch [11/20], Loss: 0.0691\n",
      "Precision: 0.8910, Recall: 0.8921, F1 Score: 0.8902\n",
      "Epoch [12/20], Loss: 0.2010\n",
      "Precision: 0.8989, Recall: 0.8930, F1 Score: 0.8945\n",
      "Epoch [13/20], Loss: 0.2933\n",
      "Precision: 0.8962, Recall: 0.8964, F1 Score: 0.8963\n",
      "Epoch [14/20], Loss: 0.3186\n",
      "Precision: 0.8957, Recall: 0.8959, F1 Score: 0.8958\n",
      "Epoch [15/20], Loss: 0.1993\n",
      "Precision: 0.8982, Recall: 0.8946, F1 Score: 0.8957\n",
      "Epoch [16/20], Loss: 0.3388\n",
      "Precision: 0.8888, Recall: 0.8896, F1 Score: 0.8871\n",
      "Epoch [17/20], Loss: 0.1200\n",
      "Precision: 0.8984, Recall: 0.8973, F1 Score: 0.8978\n",
      "Epoch [18/20], Loss: 0.4393\n",
      "Precision: 0.8989, Recall: 0.8949, F1 Score: 0.8960\n",
      "Epoch [19/20], Loss: 0.7209\n",
      "Precision: 0.9003, Recall: 0.8991, F1 Score: 0.8996\n",
      "Epoch [20/20], Loss: 0.1407\n",
      "Precision: 0.8929, Recall: 0.8941, F1 Score: 0.8924\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.init as init\n",
    "class MultiLayerPerceptron2(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_classes):\n",
    "        super(MultiLayerPerceptron2, self).__init__()\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_dim*2)\n",
    "        self.norm1 = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.fc2 = nn.Linear(hidden_dim*2, hidden_dim*2)\n",
    "        self.norm2 = nn.BatchNorm1d(hidden_dim*2)\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.dropout3 = nn.Dropout(p=0.1)\n",
    "        self.fc4 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        # Initialisation des poids\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.kaiming_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        return self.fc4(x)\n",
    "\"\"\"\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x.long())\n",
    "        embedded = self.dropout(embedded)\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        return torch.sigmoid(self.fc(hidden.squeeze(0)))\n",
    "\"\"\"\n",
    "num_epochs = 20\n",
    "vocab_size = X_tensor.shape[1]\n",
    "num_classes = len(set(y.values))\n",
    "\n",
    "model = MultiLayerPerceptron2(vocab_size, 2, num_classes) #0.87 0.87 0.87\n",
    "#model = Classifier(vocab_size, 2, vocab_size, num_classes) #\n",
    "print(model)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.long()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, Value in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_labels.extend(Value)\n",
    "            all_predictions.extend(predicted)\n",
    "\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilités de chaque classe : Humain : {}, GPT {} tensor([[6.0200e-42, 1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Calcul du vecteur de document et ajout d'une dimension\n",
    "doc_vector = document_vector(\"Je suis\", pretrained_wv)\n",
    "doc_vector = torch.tensor(doc_vector, dtype=torch.float32).unsqueeze(0)  # Ajout d'une dimension avec unsqueeze(0)\n",
    "\n",
    "# Passage au modèle\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(doc_vector)\n",
    "\n",
    "# Appliquer softmax pour obtenir des probabilités\n",
    "probabilities = F.softmax(output, dim=1)  # Utilisation de dim=1 car nous voulons appliquer softmax sur les classes\n",
    "\n",
    "# Afficher les probabilités\n",
    "print(\"Probabilités de chaque classe : Humain : {}, GPT {}\", probabilities.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
