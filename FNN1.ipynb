{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Main First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run Main.ipynb\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from glob import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_wv = gensim.downloader.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170898/170898 [04:12<00:00, 677.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def document_vector(doc, wv):\n",
    "  \"\"\"Create document vectors by averaging word vectors.\"\"\"\n",
    "  words = word_tokenize(doc)\n",
    "  word_vectors = np.array([wv[word] for word in words if word in wv])\n",
    "  \n",
    "  if len(word_vectors) == 0:\n",
    "      return np.zeros(wv.vector_size)\n",
    "  return np.mean(word_vectors, axis=0)\n",
    "  \n",
    "\n",
    "X = np.array([document_vector(text, pretrained_wv) for text in tqdm(X)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDataset objects\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron2(\n",
      "  (fc1): Linear(in_features=100, out_features=4, bias=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (fc2): Linear(in_features=4, out_features=2, bias=True)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  (fc3): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n",
      "Epoch [1/20], Loss: 0.8531\n",
      "Precision: 0.8543, Recall: 0.8557, F1 Score: 0.8507\n",
      "Epoch [2/20], Loss: 0.0963\n",
      "Precision: 0.8739, Recall: 0.8756, F1 Score: 0.8741\n",
      "Epoch [3/20], Loss: 0.3717\n",
      "Precision: 0.8799, Recall: 0.8804, F1 Score: 0.8801\n",
      "Epoch [4/20], Loss: 0.1232\n",
      "Precision: 0.8798, Recall: 0.8812, F1 Score: 0.8802\n",
      "Epoch [5/20], Loss: 0.3332\n",
      "Precision: 0.8800, Recall: 0.8816, F1 Score: 0.8800\n",
      "Epoch [6/20], Loss: 0.0794\n",
      "Precision: 0.8870, Recall: 0.8842, F1 Score: 0.8851\n",
      "Epoch [7/20], Loss: 0.1277\n",
      "Precision: 0.8864, Recall: 0.8860, F1 Score: 0.8862\n",
      "Epoch [8/20], Loss: 0.1637\n",
      "Precision: 0.8875, Recall: 0.8885, F1 Score: 0.8878\n",
      "Epoch [9/20], Loss: 0.3319\n",
      "Precision: 0.8847, Recall: 0.8860, F1 Score: 0.8841\n",
      "Epoch [10/20], Loss: 0.0703\n",
      "Precision: 0.8828, Recall: 0.8828, F1 Score: 0.8792\n",
      "Epoch [11/20], Loss: 0.4445\n",
      "Precision: 0.8892, Recall: 0.8877, F1 Score: 0.8883\n",
      "Epoch [12/20], Loss: 0.1569\n",
      "Precision: 0.8902, Recall: 0.8900, F1 Score: 0.8901\n",
      "Epoch [13/20], Loss: 0.0890\n",
      "Precision: 0.8871, Recall: 0.8883, F1 Score: 0.8862\n",
      "Epoch [14/20], Loss: 0.3573\n",
      "Precision: 0.8869, Recall: 0.8883, F1 Score: 0.8866\n",
      "Epoch [15/20], Loss: 0.2692\n",
      "Precision: 0.8887, Recall: 0.8896, F1 Score: 0.8890\n",
      "Epoch [16/20], Loss: 0.4726\n",
      "Precision: 0.8868, Recall: 0.8882, F1 Score: 0.8868\n",
      "Epoch [17/20], Loss: 0.3935\n",
      "Precision: 0.8899, Recall: 0.8905, F1 Score: 0.8901\n",
      "Epoch [18/20], Loss: 0.1753\n",
      "Precision: 0.8909, Recall: 0.8917, F1 Score: 0.8912\n",
      "Epoch [19/20], Loss: 0.1244\n",
      "Precision: 0.8870, Recall: 0.8882, F1 Score: 0.8863\n",
      "Epoch [20/20], Loss: 0.1618\n",
      "Precision: 0.8890, Recall: 0.8899, F1 Score: 0.8893\n"
     ]
    }
   ],
   "source": [
    "class MultiLayerPerceptron2(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_classes):\n",
    "        super(MultiLayerPerceptron2, self).__init__()\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_dim*2)\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.fc2 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "num_epochs = 20\n",
    "vocab_size = X_tensor.shape[1]\n",
    "num_classes = len(set(y.values))\n",
    "\n",
    "model = MultiLayerPerceptron2(vocab_size, 2, num_classes)\n",
    "print(model)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.long()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, Value in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_labels.extend(Value)\n",
    "            all_predictions.extend(predicted)\n",
    "\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilités de chaque classe : tensor([0.3181, 0.6819])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(torch.tensor(document_vector(\"Oui, les lapins peuvent être blancs ! En fait, il existe de nombreuses races de lapins qui présentent une variété de couleurs de pelage, y compris le blanc. Le lapin blanc est l'une des couleurs les plus courantes parmi les races de lapins domestiques. De plus, dans la nature, certaines espèces de lapins sauvages peuvent également avoir des variations de couleur qui incluent le blanc.\", pretrained_wv), dtype=torch.float32))\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Appliquer softmax pour obtenir des probabilités\n",
    "probabilities = F.softmax(output, dim=0)\n",
    "\n",
    "# Afficher les probabilités\n",
    "print(\"Probabilités de chaque classe :\", probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
