{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "csv = pd.read_csv(\"data/hack_train.csv\")\n",
    "csvFull = csv.rename(columns={'text': 'answers', \"label\" : \"is_human\"})\n",
    "csv = csvFull.drop(columns=\"src\")\n",
    "answers_df = csv\n",
    "\n",
    "answers_df = answers_df.explode('answers', ignore_index=True)\n",
    "answers_df = answers_df.dropna(subset=['answers'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text_column):\n",
    "    new_review = []\n",
    "    for review in text_column:\n",
    "        text = re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\", ' ', str(review).lower()).strip()\n",
    "        text = [wnl.lemmatize(i) for i in text.split(' ') if i not in stop_words]\n",
    "        new_review.append(' '.join(text))\n",
    "    return new_review\n",
    "\n",
    "answers_df[\"answers\"] = preprocess(answers_df[\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "X = answers_df[\"answers\"]\n",
    "y = answers_df[\"is_human\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "cv = TfidfVectorizer()\n",
    "cv.fit(answers_df[\"answers\"])\n",
    "X_train = cv.transform(X_train)\n",
    "X_test = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "model = xgb.XGBClassifier(n_jobs=1)\n",
    "\n",
    "\n",
    "n_estimators_list = [1, 5, 10, 20] + [20 * i for i in range(2, 11)] + [50 * i for i in range(5, 11)]\n",
    "param_grid = {\n",
    "    \"max_depth\": [6],\n",
    "    \"n_estimators\": [200, 500, 1000, 2000, 5000, 10000],\n",
    "}\n",
    "\n",
    "model = GridSearchCV(\n",
    "        model,\n",
    "        param_grid,\n",
    "        verbose=1,\n",
    "        n_jobs=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8962513199577613"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 6, 'n_estimators': 2000}\n"
     ]
    }
   ],
   "source": [
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = \"XGB_89.pickle\"\n",
    "pickle.dump(model, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(model.best_params_)\n",
    "test = cv.transform([\"The title of the oldest continuously inhabited town is often attributed to Jericho, located in the West Bank of the Palestinian Territories, with archaeological evidence suggesting settlement dating back over 11,000 years.\"])\n",
    "model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/XGB_89.pickle', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(cv.transform([\"The title of the oldest continuously inhabited town is often attributed to Jericho, located in the West Bank of the Palestinian Territories, with archaeological evidence suggesting settlement dating back over 11,000 years.\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
