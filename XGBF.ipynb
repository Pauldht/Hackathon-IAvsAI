{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "csv = pd.read_csv(\"data/hack_train.csv\")\n",
    "csvFull = csv.rename(columns={'text': 'answers', \"label\" : \"is_human\"})\n",
    "csv = csvFull.drop(columns=\"src\")\n",
    "answers_df = csv\n",
    "\n",
    "answers_df = answers_df.explode('answers', ignore_index=True)\n",
    "answers_df = answers_df.dropna(subset=['answers'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/onyxia/work/Hackathon-IAvsAI/XGBF.ipynb Cell 2\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://user-pdht-242065-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/XGBF.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mwordnet\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://user-pdht-242065-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/XGBF.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://user-pdht-242065-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/XGBF.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m stop_words \u001b[39m=\u001b[39m stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://user-pdht-242065-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/XGBF.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m wnl \u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m     <a href='vscode-notebook-cell://user-pdht-242065-0.user.lab.sspcloud.fr/home/onyxia/work/Hackathon-IAvsAI/XGBF.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess\u001b[39m(text_column):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text_column):\n",
    "    new_review = []\n",
    "    for review in text_column:\n",
    "        text = re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\", ' ', str(review).lower()).strip()\n",
    "        text = [wnl.lemmatize(i) for i in text.split(' ') if i not in stop_words]\n",
    "        new_review.append(' '.join(text))\n",
    "    return new_review\n",
    "\n",
    "answers_df[\"answers\"] = preprocess(answers_df[\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "X = answers_df[\"answers\"]\n",
    "y = answers_df[\"is_human\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "cv = TfidfVectorizer(max_features=10000)\n",
    "cv.fit(answers_df[\"answers\"])\n",
    "X_train = cv.transform(X_train)\n",
    "X_test = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparam_grid = {\\n    \"colsample_bytree\": [0.5, 0.7],\\n    \"reg_alpha\": [0.01, 0.1, 1.0],\\n    \"reg_lambda\": [0.01, 0.1, 1.0],\\n}\\n\\nmodel = GridSearchCV(\\n        model,\\n        param_grid,\\n        verbose=1,\\n        n_jobs=4,\\n    )\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "model = xgb.XGBClassifier(max_depth=6, n_estimators=2000, colsample_bytree=0.7, reg_alpha=0.01, reg_lambda=0.1)\n",
    "\n",
    "\"\"\"\n",
    "param_grid = {\n",
    "    \"colsample_bytree\": [0.5, 0.7],\n",
    "    \"reg_alpha\": [0.01, 0.1, 1.0],\n",
    "    \"reg_lambda\": [0.01, 0.1, 1.0],\n",
    "}\n",
    "\n",
    "model = GridSearchCV(\n",
    "        model,\n",
    "        param_grid,\n",
    "        verbose=1,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8788278775079198"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, verbose=1)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "#print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = \"XGB_91.pickle\"\n",
    "pickle.dump(model, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(model.best_params_)\n",
    "test = cv.transform([\"The oldest town i know is in the north of france i think\"])\n",
    "model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/XGB_90.pickle', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(cv.transform([\"The title of the oldest continuously inhabited town is often attributed to Jericho, located in the West Bank of the Palestinian Territories, with archaeological evidence suggesting settlement dating back over 11,000 years.\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
